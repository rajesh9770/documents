https://www.linkedin.com/learning/kafka-essential-training
https://data-artisans.com/blog/kafka-flink-a-practical-how-to
https://dzone.com/articles/how-to-write-a-kafka-producer-using-twitter-stream
https://github.com/twitter/hbc/blob/master/hbc-example/src/main/java/com/twitter/hbc/example/SampleStreamExample.java

============

Kafka connect allows user to share the code that source and sinks the data from common sources and sinks, such as dbms, hdfs, SAP, HANA, 
HBase, etc 
Source -> Kafa Producer API             Kafka Connect Source 
KAfka -> Kafka Consumer, Producer API   Kafka Streams
Kafka -> Sink Consumer API              Kafka Connect Sink
Kafka -> App  Consumer API


Kafka cluster consists of brokers, Connect Cluster conssits of works that pull data from sources and push it to Kafka cluster.
Stream API read/write to kafka cluster, using differnt topics.
On sinks-side Connect cluster read data from kafka cluster and write to sinks.

Kafka Connect Cluster has multiple loaded Connectors. Each connectors is a re-usable piece of code (jar file)

Connectors + User configuration =>Tasks
 1) A task is linked to a connector configuration
 2) A job configuration may spawn multiple tasks

Tasks are executed by Kafka Connect Workers(servers)
 1) A worker is a single java process
 2) A worker can be standalone or in a cluster
 3) A worker can multiple tasks from multiple connectors
 4) In a distributed mode, if one one worker dies, the tasks running on that works are rebalanced over to remaining workers


=========
Topic: A particular stream of data. Similar to a table in DB. Identified by a name.
Topics are split in partitions. Each Partioned is ordered by 1,2, 3, ..... Each msg is identified by partition it belongs to and it's number within that partition. This # is called offset. Offset has meaning for that partition.

REplication:
At any time only one broker can be a leaderfor a given partition. Only that leader can receive and serve data for a 
partition.

Producer has option of sending a key with the message. If Key is sent, then producer has guarantee that the messages with the same key always go to the same partition.

A single partion is always assigned to only one consumer. One consumer can be assigned to multiple partions if # of consumers are less than the # of partions. On the other hand, if #of cosumers are more than #of partitions, then some consumers will be inactive.

At most once. offsets are committed as soon as message is received. If processing of msg errors out, msg is lost.
At least once. offsets are comnitted after the msg is processed. If processing goes wrong, msg will be processed again.
In this case make sure processing is idempotent.
Exactly once: Hard to implement. Can be achieved for Kafka => Kafka workflows using Kafka Stream API.

=============
$KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper localhost:2181
$KAFKA_HOME/bin/kafka-topics.sh --zookeeper localhost:2181 --topic test --describe
$KAFKA_HOME/bin/kafka-topics.sh --zookeeper localhost:2181 --topic test --delete

$KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test   ## if topic does not exists it will created with defaut replication factor 1 and partition 1.
$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning --group first_app
$KAFKA_HOME/bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name test --add-config retention.ms=600000


kafkacat -L  -b 10.168.189.174
